---
description: Agent Inspector - AI Agent Security Analysis (scan, correlate static â†” dynamic)
globs: ["**/*.py", "**/*.ts", "**/*.js"]
---

# Agent Inspector Integration

**MCP Server:** `http://localhost:7100/mcp`
**Dashboard:** `http://localhost:7100`
**Proxy:** `http://localhost:4000`

## Available MCP Tools (17 total)

### Analysis Tools
| Tool | Description |
|------|-------------|
| `get_security_patterns` | Get OWASP LLM Top 10 patterns for analysis |
| `create_analysis_session` | Start session for agent workflow (requires agent_workflow_id) |
| `store_finding` | Record a security finding |
| `complete_analysis_session` | Finalize session and calculate risk score |
| `get_findings` | Retrieve stored findings |
| `update_finding_status` | Mark finding as FIXED or IGNORED |

### Knowledge Tools
| Tool | Description |
|------|-------------|
| `get_owasp_control` | Get specific OWASP control details (LLM01-LLM10) |
| `get_fix_template` | Get remediation template for a finding type |

### Agent Workflow Lifecycle Tools
| Tool | Description |
|------|-------------|
| `get_agent_workflow_state` | Check what analysis exists (static/dynamic/both) |
| `get_tool_usage_summary` | Get tool usage patterns from dynamic sessions |
| `get_agent_workflow_correlation` | Correlate static findings with dynamic runtime |

### Agent Discovery Tools
| Tool | Description |
|------|-------------|
| `get_agents` | List agents (filter by agent_workflow_id or "unlinked") |
| `update_agent_info` | Link agents to agent workflows, set display names |

### IDE Connection Tools
| Tool | Description |
|------|-------------|
| `register_ide_connection` | Register your IDE (Cursor or Claude Code) as connected |
| `ide_heartbeat` | Keep connection alive, signal active development |
| `disconnect_ide` | Disconnect IDE from Agent Inspector |
| `get_ide_connection_status` | Check current IDE connection status |

## IDE CONNECTION (REQUIRED FIRST STEP)

### Step 1: Know Your Model
**You MUST identify what AI model you are.** Common values:
- `claude-opus-4.5`, `claude-sonnet-4`, `claude-sonnet-3.5`
- `gpt-4o`, `gpt-4-turbo`
- Check your system prompt for "powered by X"

### Step 2: Register with ALL Parameters
```
register_ide_connection(
  ide_type="cursor",                      // "cursor" or "claude-code"
  agent_workflow_id="{agent_workflow_id}",    // from folder name
  workspace_path="{full_path}",           // workspace path
  model="{your_model_name}"               // e.g., "claude-opus-4.5"
)
```

### Step 3: Save connection_id
**SAVE the `connection_id` from the response!** You need it for heartbeats.

### Step 4: ONE Heartbeat (Save Tokens!)
Send **ONE** heartbeat at the start of work, NOT after every action:
```
ide_heartbeat(connection_id="{id}", is_developing=true)
```

âš ï¸ **Don't spam heartbeats** - one per task is enough!

**Dashboard indicators:**
- ðŸŸ¢ **Green**: IDE connected (lasts 60s after heartbeat)
- ðŸŸ¢âœ¨ **Pulsing green**: Actively developing

## AUTOMATIC WORKFLOW

**When user asks for security analysis, follow this flow automatically:**

### Step 1: Derive agent_workflow_id
Auto-derive from (priority order):
1. Git remote: `github.com/acme/my-agent.git` â†’ `my-agent`
2. Package name: `pyproject.toml` or `package.json`
3. Folder name: `/projects/my-bot` â†’ `my-bot`

### Step 2: Check Current State
```
get_agent_workflow_state(agent_workflow_id)
```

This returns one of:
- `NO_DATA` â†’ Run static analysis
- `STATIC_ONLY` â†’ Inform user to run dynamic tests for validation
- `DYNAMIC_ONLY` â†’ Run static analysis, then correlate
- `COMPLETE` â†’ Run correlation, report unified results

### Step 3: Discover & Link Agents (if dynamic data exists)
```
get_agents("unlinked")
```
If unlinked agents found, link them:
```
update_agent_info(agent_id, agent_workflow_id="the-agent-workflow-id")
```

### Step 4: Based on State

#### Scenario A: Static Analysis First (IDE Code Review)
This is the typical flow when user asks "run a security scan":

1. **Get Patterns**
   ```
   get_security_patterns()
   ```
   NEVER hardcode patterns - always fetch from MCP.

2. **Create Session**
   ```
   create_analysis_session(agent_workflow_id, "STATIC")
   ```

3. **Analyze Code & Store Findings**
   ```
   store_finding(session_id, file_path, finding_type, severity, title, ...)
   ```

4. **Complete Session**
   ```
   complete_analysis_session(session_id)
   ```

5. **Check for Dynamic Data**
   ```
   get_agent_workflow_state(agent_workflow_id)
   ```
   - If state is `COMPLETE`: Run correlation
   - If state is `STATIC_ONLY`: Inform user how to run dynamic tests

6. **Report Results**
   Include dashboard URL: `http://localhost:7100/agent-workflow/{agent_workflow_id}`

#### Scenario B: Dynamic Analysis First (Runtime Captured)
When user ran their agent first, then asks for security review:

1. Check state shows `DYNAMIC_ONLY`
2. Run static analysis (steps above)
3. Correlate with `get_agent_workflow_correlation(agent_workflow_id)`
4. Report which findings are VALIDATED vs UNEXERCISED

### Step 5: Correlation (when both static + dynamic exist)
```
get_agent_workflow_correlation(agent_workflow_id)
get_tool_usage_summary(agent_workflow_id)
```

Report:
- **VALIDATED**: Finding's tool was called at runtime
- **UNEXERCISED**: Tool never called in tests - needs test coverage

### Step 6: Name Agents (optional)
After analyzing code, give agents meaningful names:
```
update_agent_info(agent_id, display_name="Customer Support Bot", description="Handles booking inquiries")
```

## Dynamic Analysis Setup

Tell user to configure their agent's base_url with agent_workflow_id:

```python
# OpenAI
client = OpenAI(base_url=f"http://localhost:4000/agent-workflow/{AGENT_WORKFLOW_ID}")

# Anthropic
client = Anthropic(base_url=f"http://localhost:4000/agent-workflow/{AGENT_WORKFLOW_ID}")
```

## Fixing Issues

```
get_findings(agent_workflow_id, status="OPEN")
get_fix_template(finding_type)
# Apply fix
update_finding_status(finding_id, "FIXED", notes="Applied input validation")
```

## /scan Command

When user types "/scan [path]" or "/scan" (defaults to current workspace):

### Your Advantage Over Traditional SAST

You are smarter than any static analysis tool. You can:
- Understand code semantically, not just pattern match
- Reason about AI agent-specific vulnerabilities
- Avoid false positives through contextual understanding
- Find issues no SAST would ever catch

### How to Scan

1. Create analysis session:
   ```
   create_analysis_session(workflow_id, session_type="STATIC")
   ```

2. Get security patterns for reference (but don't limit yourself to them):
   ```
   get_security_patterns()
   ```

3. **For each code file**, analyze thoroughly looking for:

   **1. PROMPT Security (LLM01)**
   - User input concatenated into prompts without sanitization
   - System prompts that can be overridden or leaked
   - Jailbreak vectors, prompt injection points
   - Missing input validation before LLM calls
   - Prompt templates with unsafe interpolation
   
   **2. OUTPUT Security (LLM02)**
   - Agent output used directly in SQL queries, shell commands, or code
   - XSS vulnerabilities when rendering agent responses in web UI
   - Agent output passed to dangerous functions (eval, exec)
   - No output validation before downstream use
   - Unescaped agent responses in logs or displays
   
   **3. TOOL Security (LLM07, LLM08)**
   - Dangerous tools (shell, file, network) without constraints
   - Missing permission checks on tool execution
   - Tools that can be chained dangerously
   - No input validation on tool parameters
   - Insecure plugin/tool interfaces
   
   **4. DATA Security (LLM06)**
   - Hardcoded API keys, secrets, credentials
   - PII in prompts or system instructions
   - Sensitive data logged or exposed in responses
   - Credentials in error messages
   - Unencrypted sensitive data storage
   
   **5. MEMORY & CONTEXT Security**
   - Conversation history stored insecurely
   - RAG/vector store poisoning vulnerabilities
   - Context injection through retrieved documents
   - No validation of retrieved content before use
   - Unbounded context accumulation
   - Shared memory between users/sessions
   
   **6. SUPPLY CHAIN Security (LLM05)**
   - Unpinned model versions
   - External prompt sources without validation
   - Unsafe dependencies with known CVEs
   - No integrity checks on loaded resources
   - Unvalidated model downloads
   
   **7. BEHAVIORAL Security (LLM08/09)**
   - No token/cost limits
   - Unbounded loops or recursion
   - No rate limiting on tool calls
   - Missing human-in-the-loop for sensitive operations
   - Agent can be manipulated to exceed boundaries
   - No approval gates for high-risk actions

4. **For each finding**, determine:
   - Category: PROMPT, OUTPUT, TOOL, DATA, MEMORY, SUPPLY, or BEHAVIOR
   - Severity: CRITICAL, HIGH, MEDIUM, or LOW
   - OWASP LLM mapping (if applicable)
   - CWE mapping (if applicable)
   - Specific code location
   - Why it's a real issue (not a false positive)

5. Store findings:
   ```
   store_finding(session_id, file_path, finding_type, severity,
                 category, title, description, code_snippet,
                 owasp_mapping, cwe, ...)
   ```

6. Complete session:
   ```
   complete_analysis_session(session_id)
   ```

7. Report summary:
   ```
   ðŸ” AI Security Scan Complete!
   
   Scanned: 15 files
   
   Security Checks (7):
   âœ— PROMPT Security: 2 Critical issues
   âœ— OUTPUT Security: 1 High issue
   âš  TOOL Security: 2 Medium issues
   âœ“ DATA Security: Passed
   âœ“ MEMORY Security: Passed
   âœ“ SUPPLY CHAIN: Passed
   âœ“ BEHAVIORAL: Passed
   
   Gate Status: ðŸ”’ BLOCKED (2 categories failed)
   
   View details: http://localhost:7100/agent-workflow/{id}/static-analysis
   
   Fix most critical: /fix REC-001
   ```

### Quality Over Quantity

- **DO** find every real security issue
- **DO** use your understanding of context to assess severity  
- **DON'T** flag things that aren't actually exploitable
- **DON'T** generate noise like traditional SAST
- **ALWAYS** categorize into one of the 7 security checks

## /fix Command

When user types "/fix REC-XXX" or "/fix":

### With Recommendation ID: `/fix REC-001`
1. Get recommendation details:
   ```
   get_recommendation_detail(recommendation_id="REC-001")
   ```

2. Mark as fixing:
   ```
   start_fix(recommendation_id="REC-001")
   ```

3. Read affected file(s) and understand context

4. Get fix template (optional):
   ```
   get_fix_template(finding_type)
   ```

5. Apply intelligent fix:
   - Understand the INTENT of the fix
   - Adapt to specific code style
   - Handle edge cases
   - Make it idiomatic

6. Complete the fix:
   ```
   complete_fix(
     recommendation_id="REC-001",
     fix_notes="Added input sanitization using html.escape()",
     files_modified=["src/agent.py"],
     fix_method="AI_ASSISTED"
   )
   ```

7. Report result:
   ```
   âœ… Fixed REC-001: [Title]
   
   Category: [CATEGORY] ([OWASP])
   File: [file:lines]
   
   What I did:
   - [Change 1]
   - [Change 2]
   
   Next: Run /scan to verify, or /fix REC-002 to continue.
   ```

### Without ID: `/fix`
1. Get open recommendations:
   ```
   get_recommendations(workflow_id, status="PENDING", blocking_only=true)
   ```
2. Pick highest priority (CRITICAL > HIGH)
3. Follow fix flow above

### Recommendation Lifecycle

```
PENDING â†’ FIXING â†’ FIXED â†’ VERIFIED
              â†“
         DISMISSED/IGNORED
```

## Key Rules

1. **Register IDE ALWAYS** - Call `register_ide_connection` at the start of each session
2. **Retry if failed** - If registration fails, retry on EVERY subsequent MCP call until successful
3. **Send heartbeats** - Call `ide_heartbeat` before/after significant actions
4. **Auto-derive** agent_workflow_id - don't ask user
5. **Auto-link** unlinked agents via `update_agent_info`
6. **Auto-correlate** when both static + dynamic data exist
7. **Never hardcode** security patterns - always fetch from MCP
8. **Same agent_workflow_id** for static and dynamic = unified results
9. **Dashboard URL**: `http://localhost:7100/agent-workflow/{agent_workflow_id}`

## IDE CONNECTION (Only When Using Agent Inspector)

**Only connect when user asks for security analysis.** Don't connect proactively.

### Lightweight Flow (3 calls max):
```
// 1. Register (idempotent - safe to repeat)
register_ide_connection(
  ide_type="cursor",
  agent_workflow_id="{folder_name}",
  workspace_path="{path}",
  model="claude-opus-4.5"  // or your model
)

// 2. ONE heartbeat at start
ide_heartbeat(connection_id="{id}", is_developing=true)

// 3. Do all your work (scans, fixes, etc.)
// NO MORE HEARTBEATS NEEDED
```

**Skip `get_ide_connection_status`** - just register directly. Simpler.
