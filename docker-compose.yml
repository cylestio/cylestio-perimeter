version: '3.8'

services:
  llm-proxy:
    build: .
    ports:
      - "3000:3000"
    environment:
      # Basic configuration via environment variables
      - LLM_BASE_URL=${LLM_BASE_URL:-https://api.openai.com}
      - LLM_TYPE=${LLM_TYPE:-openai}
      - LLM_API_KEY=${LLM_API_KEY}
      - SERVER_PORT=3000
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    volumes:
      # Mount traces directory for persistence
      - ./traces:/app/traces
      # Mount config file if needed
      - ./config.yaml:/app/config.yaml:ro
    command: >
      sh -c "
      if [ -f /app/config.yaml ]; then
        python -m src.main --config /app/config.yaml
      else
        python -m src.main --base-url $${LLM_BASE_URL} --type $${LLM_TYPE} --api-key $${LLM_API_KEY} --port 3000 --log-level $${LOG_LEVEL}
      fi
      "
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s